[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Jake’s CS451 blog"
  },
  {
    "objectID": "posts/new-new-test-post/index.html",
    "href": "posts/new-new-test-post/index.html",
    "title": "Timnit Gebru",
    "section": "",
    "text": "from source import Perceptron\np = Perceptron()\n\nI did it!!\nnot implemented\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-new-test-post/index.html#math",
    "href": "posts/new-new-test-post/index.html#math",
    "title": "Timnit Gebru",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/example-blog-post/index.html",
    "href": "posts/example-blog-post/index.html",
    "title": "Hello Blog",
    "section": "",
    "text": "from source import Perceptron\nThis is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/example-blog-post/index.html#math",
    "href": "posts/example-blog-post/index.html#math",
    "title": "Hello Blog",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/new-test-post/index.html",
    "href": "posts/new-test-post/index.html",
    "title": "Second Post",
    "section": "",
    "text": "This is an example of the blog posts that you’ll submit as your primary form of learning demonstration in CSCI 0451. I created this post by modifying the file posts/example-blog-post/index.ipynb in VSCode. You can also use JupyterLab for this editing if you prefer. Finally, it is possible to write blog posts without using notebooks by writing .qmd files, as illustrated here."
  },
  {
    "objectID": "posts/new-test-post/index.html#math",
    "href": "posts/new-test-post/index.html#math",
    "title": "Second Post",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Awesome CSCI 0451 Blog",
    "section": "",
    "text": "Implementing Logistic Regression\n\n\n\n\n\nExploring the Logistic Regression algorithm, vanilla and spicy gradient descent and the effects of overfitting\n\n\n\n\n\nMay 5, 2024\n\n\nJake gilbert\n\n\n\n\n\n\n\n\n\n\n\n\nDeep Music Genre Classification\n\n\n\n\n\nUsing deep learning and neural networks to classify music genres\n\n\n\n\n\nMay 2, 2024\n\n\nJake gilbert\n\n\n\n\n\n\n\n\n\n\n\n\nImplementing the Perceptron Algorithm\n\n\n\n\n\nImplementing the perceptron algorithm and testing it in multiple experiments\n\n\n\n\n\nMar 25, 2024\n\n\nJake Gilbert\n\n\n\n\n\n\n\n\n\n\n\n\nClassifying Palmer Penguins\n\n\n\n\n\nClassifying Palmer Penguins based on quantitative and qualitative data\n\n\n\n\n\nFeb 22, 2024\n\n\nJake Gilbert\n\n\n\n\n\n\n\n\n\n\n\n\nSecond Post\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nTimnit Gebru\n\n\n\n\n\nA new blog post that I just made!\n\n\n\n\n\nMar 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\n\n\n\n\n\n\nHello Blog\n\n\n\n\n\nAn example blog post illustrating the key techniques you’ll need to demonstrate your learning in CSCI 0451.\n\n\n\n\n\nJan 10, 2023\n\n\nPhil Chodrow\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html",
    "href": "posts/classifying-palmer-penguins/index.html",
    "title": "Classifying Palmer Penguins",
    "section": "",
    "text": "Classifying Palmer Penguins\n\n\nAbstract\nBy using machine learning models, we can more easily classify the species of Palmer penguins given certain physical features—both quantitative and qualitative. Based on the exploration of data visualization, training and cross validation, it was concluded that a logistic regression model that was tested through cross validation using Culmen Depth (mm), Culmen Length (mm) and Island of origin (Biscoe, Dream or Torgersen) provided a 100% accurate classifier for future testing data.\nTo start the process, we need to implement the training data and our packages for data visualization.\n\nimport pandas as pd\nimport seaborn as sns\nimport numpy as np\n\n\ntrain_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/train.csv\"\ntrain = pd.read_csv(train_url)\n\ntrain.head()\n\n\n\n\n\n\n\n\n\nstudyName\nSample Number\nSpecies\nRegion\nIsland\nStage\nIndividual ID\nClutch Completion\nDate Egg\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nSex\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nComments\n\n\n\n\n0\nPAL0809\n31\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN63A1\nYes\n11/24/08\n40.9\n16.6\n187.0\n3200.0\nFEMALE\n9.08458\n-24.54903\nNaN\n\n\n1\nPAL0809\n41\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN74A1\nYes\n11/24/08\n49.0\n19.5\n210.0\n3950.0\nMALE\n9.53262\n-24.66867\nNaN\n\n\n2\nPAL0708\n4\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN32A2\nYes\n11/27/07\n50.0\n15.2\n218.0\n5700.0\nMALE\n8.25540\n-25.40075\nNaN\n\n\n3\nPAL0708\n15\nGentoo penguin (Pygoscelis papua)\nAnvers\nBiscoe\nAdult, 1 Egg Stage\nN38A1\nYes\n12/3/07\n45.8\n14.6\n210.0\n4200.0\nFEMALE\n7.79958\n-25.62618\nNaN\n\n\n4\nPAL0809\n34\nChinstrap penguin (Pygoscelis antarctica)\nAnvers\nDream\nAdult, 1 Egg Stage\nN65A2\nYes\n11/24/08\n51.0\n18.8\n203.0\n4100.0\nMALE\n9.23196\n-24.17282\nNaN\n\n\n\n\n\n\n\n\n\n\nExploring Visualizations\nIn order to accurately predict the species of penguin, we need to determine the best subset of data and features to train on. To do this, we will look at some data visualization.\nFirst, we need to prepare the data. Given the data, we will drop some columns that we will not be able to train on and focus on either quantitative data (like culmen length/depth) or one-hot encoded columns, like sex or island of origin.\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train[\"Species\"])\n\ndef prepare_data(df):\n  df = df.drop([\"studyName\", \"Sample Number\", \"Individual ID\", \"Date Egg\", \"Comments\", \"Region\"], axis = 1)\n  df = df[df[\"Sex\"] != \".\"]\n  df = df.dropna()\n  y = le.transform(df[\"Species\"])\n  df = df.drop([\"Species\"], axis = 1)\n  df = pd.get_dummies(df)\n  return df, y\n\nX_train, y_train = prepare_data(train)\n\nX_train\n\n\n\n\n\n\n\n\n\nCulmen Length (mm)\nCulmen Depth (mm)\nFlipper Length (mm)\nBody Mass (g)\nDelta 15 N (o/oo)\nDelta 13 C (o/oo)\nIsland_Biscoe\nIsland_Dream\nIsland_Torgersen\nStage_Adult, 1 Egg Stage\nClutch Completion_No\nClutch Completion_Yes\nSex_FEMALE\nSex_MALE\n\n\n\n\n0\n40.9\n16.6\n187.0\n3200.0\n9.08458\n-24.54903\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n1\n49.0\n19.5\n210.0\n3950.0\n9.53262\n-24.66867\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n2\n50.0\n15.2\n218.0\n5700.0\n8.25540\n-25.40075\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n3\n45.8\n14.6\n210.0\n4200.0\n7.79958\n-25.62618\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n4\n51.0\n18.8\n203.0\n4100.0\n9.23196\n-24.17282\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n270\n51.1\n16.5\n225.0\n5250.0\n8.20660\n-26.36863\nTrue\nFalse\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\n\n\n271\n35.9\n16.6\n190.0\n3050.0\n8.47781\n-26.07821\nFalse\nFalse\nTrue\nTrue\nTrue\nFalse\nTrue\nFalse\n\n\n272\n39.5\n17.8\n188.0\n3300.0\n9.66523\n-25.06020\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n273\n36.7\n19.3\n193.0\n3450.0\n8.76651\n-25.32426\nFalse\nFalse\nTrue\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n274\n42.4\n17.3\n181.0\n3600.0\n9.35138\n-24.68790\nFalse\nTrue\nFalse\nTrue\nFalse\nTrue\nTrue\nFalse\n\n\n\n\n256 rows × 14 columns\n\n\n\n\nTo choose the best possible features for our model, it’s helpful to visualize the dat.\nFirstly, I want to find the one hot encoded feature which doesn’t contain any numeric values. Below I’m using a box plot to visualize the the difference in the culmen depths between islands.\n\nsns.boxplot(data=train, x=\"Island\", y=\"Culmen Depth (mm)\")\n\n\n\n\n\n\n\n\nFIGURE 1: In the above figure, we can see that there is enough variance between the islands to ensure this feature could be viable. This is especially apparent on Biscoe island, which may be due to a very specific population of penguin on that island.\nWe can also see how this changes between the different species.\n\nsns.boxplot(data=train, x=\"Island\", y=\"Culmen Depth (mm)\", hue=\"Species\")\n\n\n\n\n\n\n\n\nFIGURE 2: Here we can see the reason for the large difference in Biscoe island—when separating boxes by species. Biscoe island is the only one with the Gentoo penguin, which has a significantly smaller culmen depth. This visualization also tells us that island can be a good one-hot encoded feature and an indicator of the type of penguin.\nWe can also look at some other numerical features by comparing them to culmen depth.\n\nimport matplotlib.pyplot as plt\n\nax = plt.subplots(figsize=(12, 3))\nplot = sns.boxplot(train, x = \"Body Mass (g)\", y = \"Culmen Depth (mm)\", width=0.8)\nplot.tick_params(axis='x', rotation=90)\n\n\n\n\n\n\n\n\n\nFIGURE 3: As body mass increases, culmen depth increases for a certain range, but there seems to be a threshold where it drops, perhaps due to a unique body proportion of a species.\n\nplot = sns.boxplot(data=train, x=\"Species\", y=\"Body Mass (g)\")\nplot.tick_params(labelsize=6)\n\n\n\n\n\n\n\n\nFIGURE 4: The Gentoo penguin has a larger mass, but in figure 2, it has a notably smaller culmen depth. Therefore, we see no discernable trend between body mass and culmen depth. With how variant Culmen depth is, it may be a good numeric feature.\nHowever, we also need to see if culmen depth itself could be used as a good numeric feature. Additionally, we need a second numerical feature to ensure the quality of the training. To do this we can remove the features with low variance.\nWe can select an 80% variance threshold and plug our cleaned up data frame\n\nfrom sklearn.feature_selection import VarianceThreshold\nvariance = VarianceThreshold(threshold=(.8 * (1 - .8)))\n\nvariance.fit_transform(X_train)\n\n\n\nChoosing Our Model\nWe can loop through all combinations of columns of possible features we want to use. As of now, I may want to use Island as my one-hot-encoded feature and Culmen Depth as a numerical feature, but we will iterate through all feature combinations using itertools.\nI will test logistic regression, random forest classifier and a decision tree. For each model, I will keep track of the best score for the given feature combination.\nHowever, before I determine the accuracy of the decision tree, I need to find the best max_depth parameter. To do this, I will use cross validation and an exhaustive search between two possible depth values. We will test depths 5-20 for this example with a 7 fold cross validation.\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score\n\n\ndepths = []\nmax_score = 0\nbest_depth = 0\nfor i in range(5,20):\n    classifier = DecisionTreeClassifier(max_depth=i)\n    scores = cross_val_score(estimator=classifier, X=X_train, y=y_train, cv=7, n_jobs=4)\n    if scores.mean() &gt; max_score: \n        max_score = scores.mean()\n        best_depth = i\n    depths.append((i,scores.mean()))\n\n\nprint(depths)\nprint(max_score)\nprint(best_depth)\n\n[(5, 0.960960960960961), (6, 0.9649292149292149), (7, 0.9649292149292149), (8, 0.961068211068211), (9, 0.9649292149292149), (10, 0.965036465036465), (11, 0.961068211068211), (12, 0.9649292149292149), (13, 0.965036465036465), (14, 0.9688974688974689), (15, 0.9649292149292149), (16, 0.960960960960961), (17, 0.961068211068211), (18, 0.961068211068211), (19, 0.9688974688974689)]\n0.9688974688974689\n14\n\n\nFrom the search, it seems that a max depth of 7 gives the best average score when cross validating. Now we can search through each feature combination and train our three models:\n\nfrom itertools import combinations\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n#keep track of the best score for each\nlogreg_score = 0\nlogreg_features = []\n\ndecision_score = 0\ndecision_features = []\n\nforest_score = 0\nforest_features = []\n\n\n# these are not actually all the columns: you'll \n# need to add any of the other ones you want to search for\nall_qual_cols = [\"Island\", \"Clutch Completion\"]\nall_quant_cols = ['Culmen Length (mm)', 'Culmen Depth (mm)', 'Body Mass (g)']\n\nfor qual in all_qual_cols: \n  qual_cols = [col for col in X_train.columns if qual in col ]\n  for pair in combinations(all_quant_cols, 2):\n    cols = qual_cols + list(pair) \n    \n    #logistic regression\n    LR = LogisticRegression()\n    LR.fit(X_train[cols], y_train)\n    temp_score = LR.score(X_train[cols], y_train)\n    if temp_score &gt; logreg_score:\n      logreg_score = temp_score\n      logreg_features = cols\n\n    #RANDOM FOREST\n    RF = RandomForestClassifier()\n    RF.fit(X_train[cols], y_train)\n    temp_score = RF.score(X_train[cols], y_train)\n    if temp_score &gt; forest_score:\n      forest_score = temp_score\n      forest_features = cols\n\n    #DECISION TREE\n    DT = DecisionTreeClassifier(max_depth=7)\n    DT.fit(X_train[cols], y_train)\n    temp_score = DT.score(X_train[cols], y_train)\n    if temp_score &gt; decision_score:\n      decision_score = temp_score\n      decision_features = cols\n    \n\nprint(\"Logical regression score:\", logreg_score)\nprint(\"Logical regression best feautes\", logreg_features)\nprint()\nprint(\"Random forest score: \", forest_score)\nprint(\"Random forest best features\", forest_features)\nprint()\nprint(\"Decision tree score: \", decision_score)\nprint(\"Decision tree best features\", decision_features)\n\nLogical regression score: 0.99609375\nLogical regression best feautes ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\nRandom forest score:  1.0\nRandom forest best features ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\nDecision tree score:  1.0\nDecision tree best features ['Island_Biscoe', 'Island_Dream', 'Island_Torgersen', 'Culmen Length (mm)', 'Culmen Depth (mm)']\n\n\nFrom the initial classifications, the random forest and decision trees seem to give 100% testing accuracy and all three models agree that island, culmen length and culmen depth are the best options for features. However, we need to do some cross validation to prevent over-fitting.\n\ncv_lr = cross_val_score(LR, X_train, y_train, cv=7).mean()\ncv_rf = cross_val_score(RF, X_train, y_train, cv=7).mean()\ncv_dt = cross_val_score(DT, X_train, y_train, cv=7).mean()\n\nprint(\"logistic regression: \", cv_lr)\nprint(\"Random Forest: \", cv_rf)\nprint(\"Decision Tree: \", cv_dt)\n\nlogistic regression:  1.0\nRandom Forest:  0.9882024882024882\nDecision Tree:  0.9649292149292149\n\n\nThe result of our cross validation reveals that logistic regression has a 100% accuracy. We now know we can create a reliable logistic regression model with our three features: Culmen Depth, Culmen Length and Island.\n\n\nTesting\nWe will now test our model with a new dataset, which produces a perfect prediction for the testing data.\n\ntest_url = \"https://raw.githubusercontent.com/PhilChodrow/ml-notes/main/data/palmer-penguins/test.csv\"\ntest = pd.read_csv(test_url)\n\ncolumns = ['Culmen Depth (mm)', 'Culmen Length (mm)', 'Island_Biscoe', 'Island_Dream', 'Island_Torgersen']\n\nX_test, y_test = prepare_data(test)\nLR.fit(X_train[columns], y_train)\nLR.score(X_test[columns], y_test)\n\n1.0\n\n\n\n\nPlotting Decision Regions\nNow that we have our accurate model and test data, we want to plot our decision regions.\n\nfrom matplotlib import pyplot as plt\nfrom matplotlib.patches import Patch\n\ndef plot_regions(model, X, y):\n    \n    x0 = X[X.columns[0]]\n    x1 = X[X.columns[1]]\n    qual_features = X.columns[2:]\n    \n    fig, axarr = plt.subplots(1, len(qual_features), figsize = (7, 3))\n\n    # create a grid\n    grid_x = np.linspace(x0.min(),x0.max(),501)\n    grid_y = np.linspace(x1.min(),x1.max(),501)\n    xx, yy = np.meshgrid(grid_x, grid_y)\n    \n    XX = xx.ravel()\n    YY = yy.ravel()\n\n    for i in range(len(qual_features)):\n      XY = pd.DataFrame({\n          X.columns[0] : XX,\n          X.columns[1] : YY\n      })\n\n      for j in qual_features:\n        XY[j] = 0\n\n      XY[qual_features[i]] = 1\n\n      p = model.predict(XY)\n      p = p.reshape(xx.shape)\n      \n      \n      # use contour plot to visualize the predictions\n      axarr[i].contourf(xx, yy, p, cmap = \"jet\", alpha = 0.2, vmin = 0, vmax = 2)\n      \n      ix = X[qual_features[i]] == 1\n      # plot the data\n      axarr[i].scatter(x0[ix], x1[ix], c = y[ix], cmap = \"jet\", vmin = 0, vmax = 2)\n      \n      axarr[i].set(xlabel = X.columns[0], \n            ylabel  = X.columns[1], \n            title = qual_features[i])\n      \n      patches = []\n      for color, spec in zip([\"red\", \"green\", \"blue\"], [\"Adelie\", \"Chinstrap\", \"Gentoo\"]):\n        patches.append(Patch(color = color, label = spec))\n\n      plt.legend(title = \"Species\", handles = patches, loc = \"best\")\n      \n      plt.tight_layout()\n\nplot_regions(LR, X_train[columns], y_train)\n\n\n\n\n\n\n\n\nPlotting our decision regions provides a good visualization of our model and the reasoning behind its decisions. For each island, there is enough distinction between our quantitative features to provide accurate classification.\n\n\nConfusion Matrix\nAnother way to get a good indication of our model and its accuracy is to create a confusion matrix:\n\nfrom sklearn.metrics import confusion_matrix\n\nprediction = LR.predict(X_test[columns])\n\nconfusion_matrix = confusion_matrix(y_test, prediction)\n\nprint(confusion_matrix)\n\n\n[[31  0  0]\n [ 0 11  0]\n [ 0  0 26]]\n\n\nThe result of our confusion matrix shows only values on the diagonal axis, revealing the models accuracy.\n\n\nDiscussion\nBased on my exploration of various models and features, I concluded that a logistic regression model trained with high variance features Culmen Depth, Culmen Length and one qualitative feature, Island, produces a 100% accurate model for classifying Palmer penguins. One thing that I learned from this exploration is the importance of steps like cross validation. Although on the surface, some models— for example our decision tree—we’re more accurate, cross validation reveals they were less so due to over-fitting and the nature of the dataset. I also learned the importance of data visualization in finding correct features and discerning trends in the data. Also revealing the decision regions of the model helps me understand why certain features work. There are many steps for determining the right classification model, and many of these steps can lead to interesting and surprising discoveries."
  },
  {
    "objectID": "posts/classifying-palmer-penguins/index.html#math",
    "href": "posts/classifying-palmer-penguins/index.html#math",
    "title": "Classifying Palmer Penguins",
    "section": "Math",
    "text": "Math\nIn addition to regular text using the Markdown specification, you can also write mathematics, enclosed between dollar signs. The syntax for writing math is very similar to the syntax used in the \\(\\LaTeX\\) markup language. For example, $f(x) \\approx y$ renders to \\(f(x) \\approx y\\). To place complex mathematical expressions on their own lines, use double dollar signs. For example, the expression\n$$\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2$$\nrenders to:\n\\[\\mathcal{L}(a, b) = \\sum_{i = 1}^n (ax_i + b - y_i)^2\\;.\\]\nBehind the scenes, math is powered by the MathJax engine. For more on how to write math, check this handy tutorial and quick reference."
  },
  {
    "objectID": "posts/perceptron/index.html",
    "href": "posts/perceptron/index.html",
    "title": "Implementing the Perceptron Algorithm",
    "section": "",
    "text": "%load_ext autoreload %autoreload 2 from perceptron import Perceptron, PerceptronOptimizer"
  },
  {
    "objectID": "posts/music-classification/index.html",
    "href": "posts/music-classification/index.html",
    "title": "Deep Music Genre Classification",
    "section": "",
    "text": "import torch\nimport pandas as pd\n\nimport numpy as np\n\n# for embedding visualization later\nimport plotly.express as px \nimport plotly.io as pio\n\n# for VSCode plotly rendering\npio.renderers.default = \"notebook\"\n\n# for appearance\npio.templates.default = \"plotly_white\"\n\n# for train-test split\nfrom sklearn.model_selection import train_test_split\n\n# for suppressing bugged warnings from torchinfo\nimport warnings \nwarnings.filterwarnings(\"ignore\", category = UserWarning)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nurl = \"https://raw.githubusercontent.com/PhilChodrow/PIC16B/master/datasets/tcc_ceds_music.csv\"\ndf = pd.read_csv(url)\n\n\nengineered_features = ['dating', 'violence', 'world/life', 'night/time','shake the audience','family/gospel', 'romantic', 'communication','obscene', 'music', 'movement/places', 'light/visual perceptions','family/spiritual', 'like/girls', 'sadness', 'feelings', 'danceability','loudness', 'acousticness', 'instrumentalness', 'valence', 'energy']\n\ndf.groupby(\"genre\").size()\n\ngenre\nblues      4604\ncountry    5445\nhip hop     904\njazz       3845\npop        7042\nreggae     2498\nrock       4034\ndtype: int64\n\n\n\ngenres = {\n    \"blues\": 0,\n    \"country\": 1,\n    \"hip hop\": 2,\n    \"jazz\": 3,\n    \"pop\": 4,\n    \"reggae\": 5,\n    \"rock\": 6\n}\n\ndf[\"genre\"] = df[\"genre\"].apply(genres.get)\ndf.head()\n\n\n\n\n\n\n\n\n\nUnnamed: 0\nartist_name\ntrack_name\nrelease_date\ngenre\nlyrics\nlen\ndating\nviolence\nworld/life\n...\nsadness\nfeelings\ndanceability\nloudness\nacousticness\ninstrumentalness\nvalence\nenergy\ntopic\nage\n\n\n\n\n0\n0\nmukesh\nmohabbat bhi jhoothi\n1950\n4\nhold time feel break feel untrue convince spea...\n95\n0.000598\n0.063746\n0.000598\n...\n0.380299\n0.117175\n0.357739\n0.454119\n0.997992\n0.901822\n0.339448\n0.137110\nsadness\n1.0\n\n\n1\n4\nfrankie laine\ni believe\n1950\n4\nbelieve drop rain fall grow believe darkest ni...\n51\n0.035537\n0.096777\n0.443435\n...\n0.001284\n0.001284\n0.331745\n0.647540\n0.954819\n0.000002\n0.325021\n0.263240\nworld/life\n1.0\n\n\n2\n6\njohnnie ray\ncry\n1950\n4\nsweetheart send letter goodbye secret feel bet...\n24\n0.002770\n0.002770\n0.002770\n...\n0.002770\n0.225422\n0.456298\n0.585288\n0.840361\n0.000000\n0.351814\n0.139112\nmusic\n1.0\n\n\n3\n10\npérez prado\npatricia\n1950\n4\nkiss lips want stroll charm mambo chacha merin...\n54\n0.048249\n0.001548\n0.001548\n...\n0.225889\n0.001548\n0.686992\n0.744404\n0.083935\n0.199393\n0.775350\n0.743736\nromantic\n1.0\n\n\n4\n12\ngiorgos papadopoulos\napopse eida oneiro\n1950\n4\ntill darling till matter know till dream live ...\n48\n0.001350\n0.001350\n0.417772\n...\n0.068800\n0.001350\n0.291671\n0.646489\n0.975904\n0.000246\n0.597073\n0.394375\nromantic\n1.0\n\n\n\n\n5 rows × 31 columns\n\n\n\n\n\nfrom torch.utils.data import Dataset, DataLoader\n\nclass TextDataFromDF(Dataset):\n    def __init__(self, df):\n        self.df = df\n    \n    def __getitem__(self, index):\n        return self.df.iloc[index, 5], self.df.iloc[index, 0]\n\n    def __len__(self):\n        return len(self.df)                \n\n\ndf_train, df_val = train_test_split(df,shuffle = True, test_size = 0.2)\ntrain_data = TextDataFromDF(df_train)\nval_data   = TextDataFromDF(df_val)\n\n\ntrain_data[194]\n\n('morning ride think morning ride morning ride nice ride miss morning ride longest ride morning ride morning ride morning ride morning ride slip slide go break slip slide go break matter hide send send morning ride morning ride morning ride morning ride tell ellington work jamaica buerue credit station gemini port portland morning ride morning ride morning ride morning ride',\n 64271)\n\n\n\nfrom torchtext.data.utils import get_tokenizer\nfrom torchtext.vocab import build_vocab_from_iterator\n\ntokenizer = get_tokenizer('basic_english')\n\ntokenized = tokenizer(train_data[194][0])\ntokenized\n\nOSError: dlopen(/Users/jakegilbert/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torchtext/lib/libtorchtext.so, 0x0006): Symbol not found: __ZN2at4_ops15to_dtype_layout4callERKNS_6TensorENSt3__18optionalIN3c1010ScalarTypeEEENS6_INS7_6LayoutEEENS6_INS7_6DeviceEEENS6_IbEEbbNS6_INS7_12MemoryFormatEEE\n  Referenced from: &lt;B145C7C7-A04C-3975-B142-8B160ADC1CFF&gt; /Users/jakegilbert/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torchtext/lib/libtorchtext.so\n  Expected in:     &lt;6B754090-A299-3FA1-B21D-A3C9B7051AD1&gt; /Users/jakegilbert/anaconda3/envs/ml-0451/lib/python3.9/site-packages/torch/lib/libtorch_cpu.dylib"
  },
  {
    "objectID": "posts/logistic-regression/index.html",
    "href": "posts/logistic-regression/index.html",
    "title": "Implementing Logistic Regression",
    "section": "",
    "text": "from matplotlib import pyplot as plt\n\n%load_ext autoreload\n%autoreload 2\nfrom logistic import LogisticRegression, GradientDescentOptimizer\nimport torch\n\nThe autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n\n\n\nAbstract\nThe code for the model can be found here\nIn this blog post, I implemented the popular classification algorithm Logistic Regression with both spicy and vanilla gradient descent. I conducted three experiments. In the first, I found that with no momentum (beta value) vanilla gradient descent was shown to find the optimal weight value for the model as the loss converged at its smallest value. The next experiment expanded on gradient descent by adding momentum and a faster learning rate, which kept the same accuracy, but improved the efficiency of the model’s convergence—this was an implementation of “spicy” gradient descent. I then explored the effects of overfitting. I created training and testing datasets with more dimensions (features) than data points, and the resulting accuracy showed a 100% accuracy for training data but a lower accuracy (94%) for test data. Within these experiments, I was able to see the complexities of logistic regression, its advantages of using gradient descent to find an optimal weight and understand/visualize how overfitting can hinder a model.\n\n\nExperiments\nIn order to test out logistic regression and spicy gradient decent, we need code to generate data for our classification.\nWe will generate points, wherein the number of dimensions controls the number of features and noise represents the difficulty of the problem.\n\ndef classification_data(n_points = 300, noise = 0.2, p_dims = 2):\n    \n    y = torch.arange(n_points) &gt;= int(n_points/2)\n    y = 1.0*y\n    X = y[:, None] + torch.normal(0.0, noise, size = (n_points,p_dims))\n    X = torch.cat((X, torch.ones((X.shape[0], 1))), 1)\n    \n    return X, y\n\nX, y = classification_data(noise = 0.3)\n\nWe can also plot this data:\n\ndef plot(X, y, ax):\n    targets = [0, 1]\n    for i in range(2):\n        ix = y == targets[i]\n        ax.scatter(X[ix,0], X[ix,1], s = 20,  c = y[ix], edgecolors = \"grey\", cmap = \"BrBG\", vmin = -2, vmax = 2, alpha = 0.5)\n    ax.set(xlabel = \"x1\", ylabel = \"x2\")\n\nfigure, ax = plt.subplots(1, 1)\n\n\nplot(X, y, ax)\n\n\n\n\n\n\n\n\nNow that we have the points and the algorithm implemented in logistic.py, we need to train our model:\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(10000):\n    # add other stuff to e.g. keep track of the loss over time. \n    opt.step(X, y, alpha = 0.1, beta = 0.9)\n\n\n\nVanilla Gradient Descent\nUsing our training loop, we will test and plot out the decision boundary and use a modified training loop, keeping track of the loss to achieve a correct weight vector.\nWe will keep track of the loss each time so we can graph it and visualize its progression overtime.\nWe will use 2 dimensions, a small alpha value and beta = 0:\n\nlosses = []\n\nLR = LogisticRegression() \nopt = GradientDescentOptimizer(LR)\n\nfor _ in range(10000):\n    # add other stuff to e.g. keep track of the loss over time. \n    losses.append(opt.step(X, y, alpha = 0.1, beta = 0.0))\n\nNow we can compute the accuracy by comparing our predictions to the target vector and plot the decision boundary:\n\n#plot decision region with a draw line function - courtesy of Prof Chodrow\ndef draw_line(w, x_min, x_max, ax, **kwargs):\n    w_ = w.flatten()\n    x = torch.linspace(x_min, x_max, 101)\n    y = -(w_[0]*x + w_[2])/w_[1]\n    l = ax.plot(x, y, **kwargs)\n\nfigure, ax = plt.subplots(1, 1)\nplot(X, y, ax)\nweight =  LR.w\ndraw_line(weight, -0.5, 1.5, ax)\n\n\n\n\n\n\n\n\n\nprint(\"Accuracy: \", torch.mean(1.0 * (LR.predict(X) == y)).item())\n\nAccuracy:  0.9900000095367432\n\n\nOur accuracy comes out to 0.99 with 10,000 steps in our loop. The decision boundary, given a moderate to low amount of noise seems to create a correct decision boundary between the two classifications.\nSince we kept track of the loss, we can plot it’s progression overtime:\n\n# plotting loss and final loss\nplt.figure(figsize=(12, 4))\nplt.plot(losses)\nplt.title('Vanilla Gradient Descent Loss')\nplt.xlabel('Step')\nplt.ylabel('Loss')\nplt.show()\n\nprint(\"Loss at the final step: \", losses[-1].item())\n\n\n\n\n\n\n\n\nLoss at the final step:  0.04300102964043617\n\n\nOur loss seems to be correctly decreasing with each step, slowly converging to 0. This shows our weight vector is creating the appropriate decision boundary.\n\n\nExperimenting with momentum\nUsing the same data, by using momentum (changing our beta value to 0.9 for example) can help us converge our weight vector with fewer steps than if beta = 0 (vanilla gradient descent).\nFirstly, I want to experiment with an optimal alpha value given a constant beta, then I want to compare the best alpha value with our spicy gradient descent with the vanilla gradient descent in the last experiment. The following code is the same as the last training loop, but has an outer loop that keeps track of the final loss values after 10000 iterations for alpha values ranging from 0.01 to 0.25 in 0.01 increments.\n\nlossesAlpha = []\nfinal_losses_momentum = []\n\nLR_Momentum = LogisticRegression() \nopt_momentum = GradientDescentOptimizer(LR_Momentum)\n\nfor j in range(1, 27):\n    lossesMomentum = []\n    for _ in range(10000):\n        # add other stuff to e.g. keep track of the loss over time. \n        lossesAlpha.append(opt_momentum.step(X, y, alpha = float(j) / 100, beta = 0.9))\n    final_losses_momentum.append(lossesAlpha[-1])\n\nNow we can visualize the final loss for each alpha value and see which one is optimal with beta = 0.9\n\n# plotting loss and final loss\nplt.figure(figsize=(12, 4))\nplt.plot(final_losses_momentum, marker=\"o\")\nplt.xticks([x for x in range(0, len(final_losses_momentum))])\nplt.title('Final loss after 500 iterations for different alpha values')\nplt.xlabel('alpha value (*100)')\nplt.ylabel('Final Loss')\nplt.show()\n\nprint(\"Final loss for alpha = 0.25:\", final_losses_momentum[-1].item())\n\n\n\n\n\n\n\n\nFinal loss for alpha = 0.25: 0.0396832711994648\n\n\nIt appears that after 10000 iterations, our final loss value approaches its lowest for an alpha value of 0.25, without much change after that. In this case, with a beta value of 0.9, we can increase the alpha value to 0.25, which is slightly more efficient than keeping it at 0.1 (or 10 on the graph).\nNow we will execute our training loop with 10000 iterations like before, plot the decision region, and find the accuracy\n\nlossesMomentum = []\n\nLR_Momentum = LogisticRegression() \nopt_momentum = GradientDescentOptimizer(LR_Momentum)\n\nfor _ in range(10000):\n    # add other stuff to e.g. keep track of the loss over time. \n    lossesMomentum.append(opt_momentum.step(X, y, alpha = 0.25, beta = 0.9))\n\n\nfigure, ax = plt.subplots(1, 1)\nplot(X, y, ax)\nweight =  LR_Momentum.w\ndraw_line(weight, -0.5, 1.5, ax)\n\nprint(\"Accuracy with momentum: \", torch.mean(1.0 * (LR_Momentum.predict(X) == y)).item())\n\n\nAccuracy with momentum:  0.9900000095367432\n\n\n\n\n\n\n\n\n\nWe retain the same accuracy of 99% after the same amount of iterations with our new alpha and beta values. Now we can plot the change in loss to compare the models with vanilla descent and spicy descent:\n\nplt.figure(figsize=(10, 5))\nplt.plot(lossesMomentum, label = \"Gradient descent with momentum\")\nplt.plot(losses, label=\"Vanilla gradient descent\")\nplt.legend()\nplt.title('Vanilla Gradient Descent Loss vs Spicy Gradient Descent Loss over time')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.show()\n\n\n\n\n\n\n\n\nOur plot shows that the spicy gradient descent (descent with momentum or a beta value of 0.9) converges at a faster rate than a model with vanilla gradient descent, therefore this model is faster and more efficient.\n\n\nOverfitting\nIn our last experiment, we will explore overfitting,a nd for that we need data where the number of dimensions (features) is greater than the number of points. This will show how overfitting our model on the testing data will drastically decrease the accuracy even when the training data has high accuracy.\nIn this case, I will generate a test and train data with 20 points and 100 dimensions. We will also increase the noise to make the problem harder.\n\nX_train, y_train = classification_data(n_points = 20, noise = 0.5, p_dims = 100)\nX_test, y_test = classification_data(n_points = 20, noise = 0.5, p_dims = 100)\n\nNow we can use this data to train a new model. This is the same as before, but we want to keep track of the accuracy of both the test and train model over the course of the iterations.\n\nlossesOF = []\n\nLR_OF = LogisticRegression() \nopt_OF = GradientDescentOptimizer(LR_OF)\n\n#\naccuracy_test = []\naccuracy_train = []\n\nfor _ in range(10000):\n    # add other stuff to e.g. keep track of the loss over time. \n    lossesOF.append(opt_OF.step(X_train, y_train, alpha = 0.25, beta = 0.9))\n\n    accuracy_test.append(torch.mean(1.0 * (LR_OF.predict(X_test) == y_test)))\n    accuracy_train.append(torch.mean(1.0 * (LR_OF.predict(X_train) == y_train)))\n\n\nprint(\"Final training accuracy:\", torch.mean(1.0 * (LR_OF.predict(X_train) == y_train)).item())\nprint(\"Final testing accuracy:\", torch.mean(1.0 * (LR_OF.predict(X_test) == y_test)).item())\n\n\nFinal training accuracy: 1.0\nFinal testing accuracy: 0.949999988079071\n\n\nThe final training accuracy comes out to 100%, but because of overfitting, we are given the testing accuracy decreased to 94%.\nWe can see how much this accuracy changes over our iterations:\n\nplt.plot(accuracy_test, label=\"Test data\")\nplt.plot(accuracy_train, label=\"Training data\")\nplt.legend()\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Iterations\")\n\nplt.title(\"Accuracies of overfit training and testing data over 10000 iterations with Spicy Gradient Descent\",fontsize = 10)\nplt.show()\n\n\n\n\n\n\n\n\nWith this graph, we can see how the training data is almost immediately 100% accurate, and our testing data flattens out at 94% accuracy, which shows how overfitting with too many features and not enough data for classification can skew the data and accuracy.\n\n\nDiscussion\nUsing gradient descent is a good way of calculating the descent of loss for the classification algorithm Logistic Regression. It’s process is to recalculate the weight of the model over the iterations in the training loop. While vanilla gradient is a good algorithm for finding the optimal weight, We were able to see the benefits of “momentum” as well as increasing the learning rate to make the model more efficient. Logistic Regression with Spicy Gradient Descent is a great method for classification, but by increasing the amount of dimensions and decreasing the amount of data to work with, we were able to see how this leads to overfitting with a perfect training accuracy but a subsequently lower testing accuracy.\nDuring this blog post, I learned about the implementation of a classification algorithm and how it builds upon basic Linear Models. I was also able to greater understand the benefits of using gradient descent in minimizing loss and finding the optimal weights, as well as the dangers of possibly overfitting data. While accuracy and loss is a good way to examine the accuracy of a model, it might not reveal the whole picture."
  }
]